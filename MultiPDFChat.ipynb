{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62abd643-cfee-4192-bca9-ea4810e56397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf==4.25.3\n",
      "  Using cached protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Using cached protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.5\n",
      "    Uninstalling protobuf-5.29.5:\n",
      "      Successfully uninstalled protobuf-5.29.5\n",
      "Successfully installed protobuf-4.25.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U --force-reinstall \"protobuf==4.25.3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b5e8f36-769f-4588-9cc2-3426500e19d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Module langchain_community.vectorstores not found. Please install langchain-community to access this module. You can install it using `pip install -U langchain-community`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\_api\\module_import.py:70\u001b[0m, in \u001b[0;36mcreate_importer.<locals>.import_by_name\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 70\u001b[0m     module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(new_module)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1310\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1324\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_google_genai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquestion_answering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_qa_chain\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\vectorstores\\__init__.py:186\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    185\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Look up attributes dynamically.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _import_attribute(name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain\\_api\\module_import.py:78\u001b[0m, in \u001b[0;36mcreate_importer.<locals>.import_by_name\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_module\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain_community\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     73\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install langchain-community to access this module. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can install it using `pip install -U langchain-community`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m         )\n\u001b[1;32m---> 78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: Module langchain_community.vectorstores not found. Please install langchain-community to access this module. You can install it using `pip install -U langchain-community`"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "import sys, subprocess\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\",\n",
    "                       \"google-generativeai\", \"langchain-google-genai\"])\n",
    "\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "import google.generativeai as genai\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692c22be-1576-4176-8236-17164ec8e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration & Constants   #\n",
    "\n",
    "DEFAULT_CHUNK_SIZE = 1200\n",
    "DEFAULT_CHUNK_OVERLAP = 200\n",
    "INDEX_DIR = \"faiss_index\"\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def _read_google_api_key() -> str | None:\n",
    "    key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    try:\n",
    "        key = key or st.secrets.get(\"GOOGLE_API_KEY\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00e4447-7557-4106-b812-8363e9b12ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = _read_google_api_key()  # no Streamlit UI calls at import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ed5c7-d59b-4fc9-b267-3bd19b5219ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Ingestion        \n",
    "\n",
    "def extract_text_from_pdf(file) -> str:\n",
    "    reader = PdfReader(file)\n",
    "    parts: List[str] = []\n",
    "    for page in reader.pages:\n",
    "        txt = (page.extract_text() or \"\").replace(\"\\x00\", \" \")\n",
    "        parts.append(txt)\n",
    "    return \"\\n\".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13c9b4f-f9f7-404e-9352-036c7f8cbd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_files(files: List[\"UploadedFile\"]) -> Tuple[List[str], List[dict]]:\n",
    "    texts: List[str] = []\n",
    "    metadatas: List[dict] = []\n",
    "    chunk_size = st.session_state.get(\"chunk_size\", DEFAULT_CHUNK_SIZE)\n",
    "    chunk_overlap = st.session_state.get(\"chunk_overlap\", DEFAULT_CHUNK_OVERLAP)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n",
    "    for f in files:\n",
    "        name = getattr(f, \"name\", \"document\")\n",
    "        if name.lower().endswith(\".pdf\"):\n",
    "            raw = extract_text_from_pdf(f)\n",
    "        else:\n",
    "            raw = f.read().decode(errors=\"ignore\")\n",
    "\n",
    "        for i, chunk in enumerate(splitter.split_text(raw)):\n",
    "            texts.append(chunk)\n",
    "            metadatas.append({\"source\": name, \"chunk_id\": i})\n",
    "\n",
    "    return texts, metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae117305-c23d-4b5e-9b54-6936be33ee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Store               \n",
    "\n",
    "def build_index(texts: List[str], metadatas: List[dict]) -> None:\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    vs = FAISS.from_texts(texts, embedding=embeddings, metadatas=metadatas)\n",
    "    vs.save_local(INDEX_DIR)\n",
    "\n",
    "def load_index() -> FAISS:\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    return FAISS.load_local(INDEX_DIR, embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ce3154-2599-45b3-8f4b-563158ba1841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA Chain                  \n",
    "\n",
    "def make_qa_chain(temperature: float = 0.2):\n",
    "    prompt = PromptTemplate(\n",
    "        template=(\n",
    "            \"Answer ONLY with the information in the context.\\n\"\n",
    "            \"If the answer is not present, reply exactly:\\n\"\n",
    "            \"\\\"I don't have that in the provided documents.\\\"\\n\\n\"\n",
    "            \"Context:\\n{context}\\n\\n\"\n",
    "            \"Question:\\n{question}\\n\\n\"\n",
    "            \"Answer:\"\n",
    "        ),\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=temperature)\n",
    "    return load_qa_chain(llm, chain_type=\"stuff\", prompt=prompt)\n",
    "\n",
    "def answer_query(query: str, top_k: int = 4, temperature: float = 0.2) -> Tuple[str, List[dict]]:\n",
    "    if not os.path.isdir(INDEX_DIR):\n",
    "        raise FileNotFoundError(\"No index found. Upload files and click 'Submit & Build Index' first.\")\n",
    "    vector_store = load_index()\n",
    "    docs = vector_store.similarity_search(query, k=top_k)\n",
    "    chain = make_qa_chain(temperature=temperature)\n",
    "    result = chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "    answer = result.get(\"output_text\", \"\").strip()\n",
    "\n",
    "    sources = []\n",
    "    for d in docs:\n",
    "        meta = {\"source\": d.metadata.get(\"source\"), \"chunk_id\": d.metadata.get(\"chunk_id\")}\n",
    "        snippet = d.page_content[:300].replace(\"\\n\", \" \")\n",
    "        meta[\"snippet\"] = snippet + (\"...\" if len(d.page_content) > 300 else \"\")\n",
    "        sources.append(meta)\n",
    "\n",
    "    return answer, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df3b657-23e8-4eb5-a724-9011d582e26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlit UI                #\n",
    "\n",
    "def sidebar_controls():\n",
    "    with st.sidebar:\n",
    "        st.header(\"‚öôÔ∏è Settings\")\n",
    "        st.session_state[\"chunk_size\"] = st.number_input(\n",
    "            \"Chunk size\", min_value=200, max_value=8000, value=DEFAULT_CHUNK_SIZE, step=100\n",
    "        )\n",
    "        st.session_state[\"chunk_overlap\"] = st.number_input(\n",
    "            \"Chunk overlap\", min_value=0, max_value=4000, value=DEFAULT_CHUNK_OVERLAP, step=50\n",
    "        )\n",
    "        top_k = st.slider(\"Top-K retrieval\", min_value=1, max_value=10, value=4)\n",
    "        temperature = st.slider(\"Answer temperature\", min_value=0.0, max_value=1.0, value=0.2, step=0.1)\n",
    "\n",
    "        st.markdown(\"---\")\n",
    "        st.subheader(\"üìÅ Documents\")\n",
    "        with st.form(\"ingest\"):\n",
    "            uploads = st.file_uploader(\n",
    "                \"Upload PDF/TXT files and click **Submit & Build Index**\",\n",
    "                type=[\"pdf\", \"txt\"], accept_multiple_files=True\n",
    "            )\n",
    "            submitted = st.form_submit_button(\"Submit & Build Index\")\n",
    "        if submitted:\n",
    "            if not API_KEY:\n",
    "                st.error(\"Missing GOOGLE_API_KEY. Set it in .env or Streamlit secrets before building the index.\")\n",
    "                st.stop()\n",
    "            with st.spinner(\"Indexing‚Ä¶\"):\n",
    "                texts, metas = ingest_files(uploads or [])\n",
    "                if not texts:\n",
    "                    st.error(\"No text extracted. Check your files.\")\n",
    "                    st.stop()\n",
    "                build_index(texts, metas)\n",
    "                st.success(f\"Index built with {len(texts)} chunks ‚úÖ\")\n",
    "\n",
    "        st.markdown(\"---\")\n",
    "        if st.button(\"üóëÔ∏è Clear local index\"):\n",
    "            import shutil\n",
    "            if os.path.isdir(INDEX_DIR):\n",
    "                shutil.rmtree(INDEX_DIR, ignore_errors=True)\n",
    "                st.success(\"Cleared local index.\")\n",
    "            else:\n",
    "                st.info(\"No index to clear.\")\n",
    "\n",
    "        return top_k, temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cda8ae0-e0e9-4ef8-b617-c30673b3ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_page():\n",
    "    # IMPORTANT: first Streamlit call\n",
    "    st.set_page_config(page_title=\"Multi-Document Chat\", page_icon=\"üìö\", layout=\"centered\")\n",
    "    st.title(\"Multi-Document Chat üìöü§ñ\")\n",
    "    st.caption(\"Ask questions across PDFs/TXT with RAG (Gemini + FAISS).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93d4560-abb2-4a5b-8cf0-0c6ea52f6e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    init_page()\n",
    "\n",
    "    # Configure Gemini AFTER set_page_config so we can safely show warnings/errors\n",
    "    if API_KEY:\n",
    "        genai.configure(api_key=API_KEY)\n",
    "    else:\n",
    "        st.warning(\"GOOGLE_API_KEY not found. Set it in a .env or Streamlit secrets to enable embeddings/LLM.\")\n",
    "\n",
    "    top_k, temperature = sidebar_controls()\n",
    "\n",
    "    st.markdown(\"### üîé Ask a question\")\n",
    "    query = st.text_input(\"Type your question about the uploaded documents:\")\n",
    "\n",
    "    if query:\n",
    "        if not API_KEY:\n",
    "            st.error(\"Missing GOOGLE_API_KEY. Set it in .env or Streamlit secrets.\")\n",
    "            st.stop()\n",
    "        try:\n",
    "            answer, source_list = answer_query(query, top_k=top_k, temperature=temperature)\n",
    "        except FileNotFoundError as e:\n",
    "            st.warning(str(e))\n",
    "            st.stop()\n",
    "        except Exception as e:\n",
    "            st.error(f\"Something went wrong while answering: {e}\")\n",
    "            st.stop()\n",
    "\n",
    "        st.markdown(\"#### üí¨ Answer\")\n",
    "        st.write(answer or \"No answer returned.\")\n",
    "\n",
    "        with st.expander(\"üìé Sources\"):\n",
    "            for i, src in enumerate(source_list, start=1):\n",
    "                st.markdown(\n",
    "                    f\"**{i}. {src.get('source','unknown')} ‚Äî chunk {src.get('chunk_id','?')}**\\n\\n\"\n",
    "                    f\"{src.get('snippet','')}\"\n",
    "                )\n",
    "\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        <hr/>\n",
    "        <div style=\"text-align:center; opacity:0.7;\">\n",
    "            ¬© Made by <a href=\"https://github.com/your-handle\" target=\"_blank\">you</a>.\n",
    "        </div>\n",
    "        \"\"\",\n",
    "        unsafe_allow_html=True,\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
